# -*- coding: utf-8 -*-
"""Modeling & Evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aLCTLDqBQ8a_iEQYDiXLieRa34Vc4Skz
"""

import pandas as pd
import numpy as np
from scipy.stats import chi2_contingency
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

import pandas as pd

# Charger les datasets
train_path = "data/train_clean.csv"  # Remplace par ton chemin rÃ©el
test_path = "data/test_clean.csv"    # Remplace par ton chemin rÃ©el

train_clean = pd.read_csv(train_path)
test__clean = pd.read_csv(test_path)

# 1ï¸âƒ£ Afficher les 5 premiÃ¨res lignes
print("\nðŸ”¹ AperÃ§u des 5 premiÃ¨res lignes du dataset d'entraÃ®nement (TR) :")
print(train_clean.head())

print("\nðŸ”¹ AperÃ§u des 5 premiÃ¨res lignes du dataset de test (TS) :")
print(test__clean.head())

# 2ï¸âƒ£ Obtenir les dimensions des datasets
print("\nðŸ“ Dimensions des datasets :")
print(f"data/train: {train_clean.shape}, Test: {test__clean.shape}")

# 3ï¸âƒ£ Identifier les types de variables
print("\nðŸ” Types de variables dans le dataset d'entraÃ®nement (TR) :")
print(train_clean.dtypes)

print("\nðŸ” Types de variables dans le dataset de test (TS) :")
print(test__clean.dtypes)

print("\nâœ… Exploration terminÃ©e avec succÃ¨s !")

# SÃ©lectionner uniquement les colonnes numÃ©riques
numeric_columns = train_clean.select_dtypes(include=['float64', 'int64']).columns

# Calculer la matrice de corrÃ©lation sur les colonnes numÃ©riques
corr_matrix = train_clean[numeric_columns].corr()

# Identification des paires de variables avec une corrÃ©lation supÃ©rieure Ã  0.9
high_corr_var = [(i, j) for i in corr_matrix.columns for j in corr_matrix.columns if i != j and corr_matrix.loc[i, j] > 0.9]
print(high_corr_var)

# SÃ©lectionner uniquement les colonnes numÃ©riques
numeric_columns = train_clean.select_dtypes(include=['float64', 'int64']).columns

# Calculer la matrice de corrÃ©lation sur les colonnes numÃ©riques
corr_matrix = train_clean[numeric_columns].corr()

# Affichage de la matrice de corrÃ©lation
print(corr_matrix)

# SÃ©lection des variables catÃ©gorielles
cat_columns = train_clean.select_dtypes(include=['object']).columns

# Test du khi-deux pour chaque paire de variables catÃ©gorielles
chi2_p_values = {}
for col1 in cat_columns:
    for col2 in cat_columns:
        if col1 != col2:
            contingency_table = pd.crosstab(train_clean[col1], train_clean[col2])
            if contingency_table.size > 0:  # Ensure the contingency table is not empty
                chi2, p, dof, expected = chi2_contingency(contingency_table)
                if p < 0.05:
                    chi2_p_values[(col1, col2)] = p
            else:
                print(f"Contingency table for {col1} and {col2} is empty, skipping chi-square test.")

# Affichage des p-values
print("P-values des tests du khi-deux pour les paires de variables catÃ©gorielles :")
for pair, p_value in chi2_p_values.items():
    print(f"{pair}: p-value = {p_value}")

# SÃ©lection des variables numÃ©riques
num_columns = train_clean.select_dtypes(include=['float64', 'int64']).columns

# Handle NaN values by imputing with the mean of each column
train_clean[num_columns] = train_clean[num_columns].apply(lambda x: x.fillna(x.mean()), axis=0)

# Standardisation des donnÃ©es
scaler = StandardScaler()
train_scaled = scaler.fit_transform(train_clean[num_columns])

# Application de la PCA
pca = PCA()
train_pca = pca.fit_transform(train_scaled)

# Variance expliquÃ©e par chaque composante principale
explained_variance = pca.explained_variance_ratio_

# Affichage de la variance expliquÃ©e cumulÃ©e
cumulative_variance = explained_variance.cumsum()
print("Variance expliquÃ©e cumulÃ©e par les composantes principales :")
print(cumulative_variance)

import matplotlib.pyplot as plt
import seaborn as sns

# Affichage de l'histogramme de la variable cible
plt.figure(figsize=(10, 6))
sns.histplot(train_clean['saleprice'], kde=True, color='blue', bins=30)
plt.title('Distribution de la variable cible : SalePrice')
plt.xlabel('saleprice')
plt.ylabel('FrÃ©quence')
plt.show()

import matplotlib.pyplot as plt

# Affichage de l'histogramme de la variable cible
train_clean['saleprice'].plot(kind='hist', bins=50, edgecolor='black')
plt.title('Distribution de saleprice')
plt.xlabel('saleprice')
plt.ylabel('FrÃ©quence')
plt.show()

print(train_clean.dtypes)

# Identification des colonnes de type 'object'
colonnes_categorielles = train_clean.select_dtypes(include=['object']).columns
print("Colonnes catÃ©gorielles :", colonnes_categorielles)

# Encodage one-hot pour les variables catÃ©gorielles
train_clean = pd.get_dummies(train_clean, columns=colonnes_categorielles, drop_first=True)

print(train_clean.dtypes)

colonnes_non_numeriques = train_clean.select_dtypes(include=['object']).columns
print("Colonnes non numÃ©riques :", colonnes_non_numeriques)

import matplotlib.pyplot as plt
import seaborn as sns

# Afficher la distribution de la variable cible 'saleprice'
plt.figure(figsize=(10,6))
sns.histplot(train_clean['saleprice'], kde=True)
plt.title('Distribution de saleprice')
plt.xlabel('Saleprice')
plt.ylabel('FrÃ©quence')
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.over_sampling import RandomOverSampler

# DÃ©finir 3 groupes : "Bas", "Moyen", "Ã‰levÃ©"
train_clean["Price_Category"] = pd.qcut(train_clean["saleprice"], q=3, labels=["Bas", "Moyen", "Ã‰levÃ©"])

# SÃ©parer X et y
X = train_clean.drop(columns=["id", "saleprice", "Price_Category"])
y = train_clean["Price_Category"]

# Appliquer le surÃ©chantillonnage
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X, y)

# Afficher la nouvelle distribution aprÃ¨s surÃ©chantillonnage
plt.figure(figsize=(8,5))
sns.countplot(x=y_resampled)
plt.title("Distribution des catÃ©gories de SalePrice aprÃ¨s surÃ©chantillonnage")
plt.xlabel("CatÃ©gorie de prix")
plt.ylabel("Nombre d'exemples")
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

# SÃ©parer les donnÃ©es en train et test (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Initialiser le modÃ¨le RandomForestClassifier
rf_model = RandomForestClassifier(random_state=42)

# EntraÃ®ner le modÃ¨le
rf_model.fit(X_train, y_train)

# Faire des prÃ©dictions sur l'ensemble de test
y_pred = rf_model.predict(X_test)

# Ã‰valuer les performances du modÃ¨le
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy du modÃ¨le Random Forest (AprÃ¨s surÃ©chantillonnage ) : {accuracy:.2f}")

# Afficher un rapport de classification pour Ã©valuer les performances sur chaque catÃ©gorie
print("Classification Report :")
print(classification_report(y_test, y_pred))

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

# SÃ©parer les donnÃ©es en train et test (80% train, 20% test)
X = train_clean.drop(columns=["id", "saleprice", "Price_Category"])  # Les features
y = train_clean["Price_Category"]  # La cible

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialiser le modÃ¨le RandomForestClassifier
rf_model_before_resampling = RandomForestClassifier(random_state=42)

# EntraÃ®ner le modÃ¨le sur les donnÃ©es avant le surÃ©chantillonnage
rf_model_before_resampling.fit(X_train, y_train)

# Faire des prÃ©dictions sur l'ensemble de test
y_pred_before = rf_model_before_resampling.predict(X_test)

# Ã‰valuer les performances du modÃ¨le
accuracy_before = accuracy_score(y_test, y_pred_before)
print(f"Accuracy du modÃ¨le Random Forest (avant surÃ©chantillonnage) : {accuracy_before:.2f}")

# Afficher un rapport de classification pour Ã©valuer les performances sur chaque catÃ©gorie
print("Classification Report (avant surÃ©chantillonnage) :")
print(classification_report(y_test, y_pred_before))

y_pred_before = rf_model_before_resampling.predict(X_test)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Matrice de confusion avant surÃ©chantillonnage
cm_before = confusion_matrix(y_test, y_pred_before)

# Affichage de la matrice de confusion avant surÃ©chantillonnage
plt.figure(figsize=(8, 6))
sns.heatmap(cm_before, annot=True, fmt="d", cmap="Blues", xticklabels=["Bas", "Moyen", "Ã‰levÃ©"], yticklabels=["Bas", "Moyen", "Ã‰levÃ©"])
plt.title("Matrice de confusion - Avant surÃ©chantillonnage")
plt.xlabel("PrÃ©dictions")
plt.ylabel("VÃ©ritables")
plt.show()

# Matrice de confusion aprÃ¨s surÃ©chantillonnage
cm_after = confusion_matrix(y_test, y_pred)

# Affichage de la matrice de confusion aprÃ¨s surÃ©chantillonnage
plt.figure(figsize=(8, 6))
sns.heatmap(cm_after, annot=True, fmt="d", cmap="Blues", xticklabels=["Bas", "Moyen", "Ã‰levÃ©"], yticklabels=["Bas", "Moyen", "Ã‰levÃ©"])
plt.title("Matrice de confusion - AprÃ¨s surÃ©chantillonnage")
plt.xlabel("PrÃ©dictions")
plt.ylabel("VÃ©ritables")
plt.show()

# Pour le modÃ¨le avant surÃ©chantillonnage
print(f"Accuracy avant surÃ©chantillonnage : {accuracy_before:.2f}")
print("Rapport de classification avant surÃ©chantillonnage :")
print(classification_report(y_test, y_pred_before))

# Pour le modÃ¨le aprÃ¨s surÃ©chantillonnage
accuracy_after = accuracy_score(y_test, y_pred)
print(f"Accuracy aprÃ¨s surÃ©chantillonnage : {accuracy_after:.2f}")
print("Rapport de classification aprÃ¨s surÃ©chantillonnage :")
print(classification_report(y_test, y_pred))

"""Avant surÃ©chantillonnage :
Accuracy : 90%
Classe "Bas" : Recall 95%, F1-score 91%
Classe "Moyen" : Recall 83%, F1-score 87%
Classe "Ã‰levÃ©" : Recall 95%, F1-score 94%
Observation : Le modÃ¨le semble bien fonctionner, mais il favorise peut-Ãªtre les classes majoritaires.

AprÃ¨s surÃ©chantillonnage :
Accuracy : 81% (baisse de 9%)
Classe "Bas" : Recall 91% (+4%), F1-score 83%
Classe "Moyen" : Recall 66% (-17%), F1-score 73%
Classe "Ã‰levÃ©" : Recall 89% (-6%), F1-score 88%
Observation :
Le modÃ¨le est plus Ã©quilibrÃ© entre les classes.
Il est meilleur pour la classe "Bas" (meilleur recall).
Il a plus de difficultÃ© avec la classe "Moyen" (baisse du recall).
"""

